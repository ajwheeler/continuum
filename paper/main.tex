% Copyright 2023 Andy Casey (Monash) and friends
% TeX magic by David Hogg (NYU)

\documentclass[modern]{aastex631}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\renewcommand{\twocolumngrid}{}
\addtolength{\topmargin}{-0.35in}
\addtolength{\textheight}{0.6in}
\setlength{\parindent}{3.5ex}
\renewcommand{\paragraph}[1]{\medskip\par\noindent\textbf{#1}~---}

% figure setup
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[framemethod=tikz]{mdframed}
\usetikzlibrary{shadows}
\definecolor{captiongray}{HTML}{555555}
\mdfsetup{%
innertopmargin=2ex,
innerbottommargin=1.8ex,
linecolor=captiongray,
linewidth=0.5pt,
roundcorner=1pt,
shadow=false,
}
\newlength{\figurewidth}
\setlength{\figurewidth}{0.75\textwidth}

% text macros
\shorttitle{Stellar continuum modelling}
\shortauthors{Casey}
\newcommand{\documentname}{\textsl{Article}}
\newcommand{\sectionname}{Section}

\newcommand{\project}[1]{\textit{#1}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\vectheta}{\boldsymbol{\theta}}
\newcommand{\vecpsi}{\boldsymbol{\psi}}
\newcommand{\vecW}{\mathbf{W}}
\newcommand{\vecH}{\mathbf{H}}
\newcommand{\vecX}{\mathbf{X}}
\newcommand{\hadamard}{\odot}
\newcommand{\apogee}{\project{APOGEE}}
\newcommand{\boss}{\project{BOSS}}

% math macros
\newcommand{\unit}[1]{\mathrm{#1}}
\newcommand{\mps}{\unit{m\,s^{-1}}}
\newcommand{\kmps}{\unit{km\,s^{-1}}}
\newcommand{\transpose}{^\top}


% notes
\definecolor{tab:blue}{HTML}{1170aa}
\newcommand{\todo}[1]{\textcolor{tab:blue}{#1}}

\sloppy\sloppypar\raggedbottom\frenchspacing
\begin{document}

\title{\Huge Stellar continuum modelling}

\author[0000-0003-0174-0564]{Andrew R. Casey}
\affiliation{School of Physics \& Astronomy, Monash University}
\affiliation{Centre of Excellence for Astrophysics in Three Dimensions (ASTRO-3D)}

%\author[0000-0003-2866-9403]{David W. Hogg}
%\affiliation{Center for Cosmology and Particle Physics, Department of Physics, New York University}
%\affiliation{Max-Planck-Institut f\"ur Astronomie, Heidelberg}
%\affiliation{Flatiron Institute, a division of the Simons Foundation}


\begin{abstract}\noindent
Continuum normalization is often a necessary step when analyzing stellar spectra. 
%The best approach is to forward model the continuum (and instrument response) simultaneously with the stellar parameters, but continuum normalization is usually required before estimating the stellar parameters.
We present a forward model to simultaneously fit stellar absorption and the joint continuum-instrument response using two linear models: a non-negative matrix factorization to approximate line absorption,
% in theoretical (normalized) spectra, 
and a sines-and-cosines basis for the joint continuum-instrument response.
The non-negative matrix factorization ensures that basis spectra are strictly additive, thereby restricting the predicted normalized flux to be at most 1, with the sines-and-cosines basis to describe the rest.
The linearity of both components ensures that inference is stable and fast.
We describe three variants of this approach:
    (1) a model-driven approach where line absorption is factorized entirely from continuum-normalized theoretical spectra;
    (2) a purely data-driven approach where line absorption and continuum are fit entirely from data;
    and
    (3) a hybrid approach where basis spectra are initialized from factorization of theoretical spectra, but iteratively adjusted to fit data. 
We apply these methods to \project{Sloan Digital Sky Survey} optical and infrared spectra of stars spanning evolutionary stages from pre-main-sequence stars to white dwarfs.
We find good fits to data, even for the model-driven variant where theoretical spectra used to factorize line absorption is provably incorrect for some features.
We show that the basis amplitudes can be used to reliably estimate stellar parameters and chemical abundances, 
and that we obtain low bias estimates of the continuum as a function of stellar parameters and signal-to-noise ratios.
%This approach largely eliminates subjectivity in stellar continuum normalization.
\end{abstract}

\keywords{Some --- keywords --- here}

\section*{}\clearpage
\section{Introduction}\label{sec:intro}

Continuum normalization is a subject
Continuum normalisation is a necessary step

Often needed to be done before stellar parameter determination, as a step towards stellar parameter determination

For measuring equivalent widths, or for full-spectrum fitting, or even for performing radial velocity determination

Many methods. Some done by eye.

Fitting the continuum well requires you to know where the line absorption is. But knowing where the line absorption is requires you to know the stellar parameters.

Best is to fit with the stellar parameters, but is intractable.

For some stars there is NO continuum (e.g., M-dwarfs). 

cite cases of NMF use in astronomy (not that many)


\section{Methods}\label{sec:methods}

We will assume a forward model for the data that includes two components: a component to represent continuum-normalized absorption (e.g., line absorption), and a component to represent the smooth continuum.\footnote{Most stellar spectra are not flux-calibrated; the continuum and instrument response enter multiplicatively and require some work to disentangle the two. Throughout this paper when we refer to the continuum, we mean the joint continuum-instrument response.} We will assume both components to be linear models. This is not a strict requirement, but keeping linearity ensures that  inference is fast and stable, and in practice the linear models we construct seem sufficient to model stellar spectra for the purposes of continuum normalisation.\\

% other assumptions
% - NMF is sufficient for representing continuum-normalized spectra
% - that the theoretical models are OK enough. there are ways they can be wrong (eg hydrogen lines) and still work in practice, or there are ways they can be wrong entirely (eg missing molecular band) and not work at all
% - that sines and cosines are sufficiently flexible for modelling the large scale continuum
The problem can be described as having data that are a one-dimensional spectrum with $P$ pixels, each with wavelength $\lambda_i$, flux $y_i$, and flux error $\sigma_i$ (with $1 \leq i \leq P$). The forward model for these data can be expressed as the element-wise multiplication of the line absorption model $f(\lambda_i; \vecpsi)$ and the continuum-instrument response $g(\lambda_i;\vectheta)$
\begin{align}
    y_i &= f(\lambda_i;\vecpsi)\hadamard{}g(\lambda_i;\vectheta) + \mbox{noise}
\end{align}
where $\hadamard$ represents the element-wise (Hadamard) product and the components $f(...)$ and $g(...)$ are described below.\\


% NMF first
The line absorption model $f(\lambda_i;\vecpsi)$ is constructed from a grid of $N$ theoretical spectra each with $D$  continuum-normalized fluxes, which we denote as $\vec{M}$. In the applications that we present, the data are assumed to have the same wavelength sampling and a consistent line spread function, such that we use a grid where $D = P$. Other situations are permitted, but at inference time there is a need to interpolate the line absorption to the $P$ observed pixels. With our $\vec{M}$ matrix of continuum-normalized theoretical flux values, we then defined \emph{stellar absorption} $\vecX$ to be
\begin{align}
    \vecX = 1 - \vec{M}
\end{align}
such that $\vecX \in \left[0, 1\right)$ and is zero when no line absorption exists. This is a `trick' that allows us to construct a highly constrained and sparse approximation to the matrix $\vecX$ using non-negative matrix factorization (NMF) such that 
\begin{align}
    \vecX \approx \vec{W}\vec{H} \label{eq:nmf}
\end{align}
where all elements in $\vecX$, $\vec{W}$, and $\vec{H}$ are required to be non-negative. 
We could construct an approximation directly to $\vec{M}$, but this would lead to denser matrices: there are \emph{no} elements of $\vec{M}$ that are exactly 0 (i.e., total line absorption), but there are \emph{many} elements in $\vecX$ that are 0 (i.e., no line absorption). For this approximation we must select the number of components $C$ to use, which is significantly smaller than both the number of theoretical spectra $N$ and the number of pixels $D$ per theoretical spectrum. Here $\vec{W}$ is a $N \times C$ matrix that can be thought of as $C$ amplitudes per theoretical spectrum, and $\vec{H}$ is a $C \times D$ matrix of $C$ corresponding eigenspectra. Figure~\ref{fig:schematic} illustrates the NMF procedure and shows some example infrared eigenspectra $\vec{H}$. \\

\begin{figure*}
    \caption{A schematic illustrating the non-negative matrix factorization procedure, with some example infrared eigenspectra. \label{fig:schematic}}
\end{figure*}

\noindent{}With the matrix $\vec{H}$ we can now define the line absorption function $f(\lambda_i;\vecpsi)$ 
\begin{align}
    f(\lambda_i;\vecpsi) = 1 - \vecpsi\vecH \label{eq:f}
\end{align}
where $\vecpsi \in [0, \infty)$ is a row vector of $C$ elements. $\vecpsi$ is analogous to a single row in $\vecW$: it represents the $C$ amplitudes needed to reconstruct the stellar absorption from the $C$ eigenspectra in $\vecH$. Note that because $\vecpsi$ and $\vecH$ are both restricted to have non-negative elements, Equation~\ref{eq:f} shows that the maximum value that can be predicted by $f(...)$ is 1. This restricts the flexibility of $f(...)$ to only be able to model continuum-normalized flux values, leaving $g(...)$ to represent the joint continuum-instrument response. While negative continuum-normalized flux values are allowed by Equation~\ref{eq:f} (i.e., $\vecpsi\vecH > 1$), this would be readily compensated by a negative continuum model $g(...) < 0$ and in practice such local minima are unfavoured.\\

There are many suitable choices for the continuum-instrument response model $g(\lambda_i;\vectheta)$. Here we chose a sine-and-cosine basis because it is a linear representation, and is sufficiently flexible for modelling the joint continuum-instrument response across a variety of spectrographs. The component $g(\lambda_i;\vectheta)$ is expressed as compactly as
\begin{align}
    g(\lambda_i;\vectheta) = \vec{A}(\lambda_i)\vectheta
\end{align}
where $\vec{A}(\lambda_i)$ returns a design matrix where the elements of the $j$th column are,
\begin{align}
    \vec{A}_{j}(\lambda_i) & = \left\{\begin{array}{cl}\displaystyle\cos\left(\frac{\pi\,[j-1]}{L}\,\lambda_i\right) & \mbox{for $j$ odd} \\[3ex]
                                       \displaystyle\sin\left(\frac{\pi\,j}{L}\,\lambda_i\right) & \mbox{for $j$ even}\end{array}\right. ~,
\end{align}
\noindent{}and the design matrix $\vec{A}(\vec{\lambda})$ can be constructed \emph{a priori} before inference begins.\\

%In later sections we describe applications of our method to real data. For now we will describe a few options that we found to work reasonably well across all settings, which we have since established as default behaviour in the accompanying software implementation. When faced with the choice of how many spectra to include when performing non-negative matrix factorization, we found good results by including everything that could fit into memory. Using limited precision floats (e.g., float-8) helped. Initialising with non-negative double singular value decomposition (with zeros filled with small random values) seemed to work very well. Multiplicative updates.

% put this in discussion
%As we discuss in Section~\ref{sec:discussion}, this is not true of all linear models: it is a design choice that leads to this strict constraint. Other linear models (e.g., PCA) allow for summation of large positive and negative amplitudes, leading to  of positive and negative eigenspectra, 

\subsection{Constructing the line absorption model}

The line absorption model $f(\lambda_i;\vecpsi)$ can be easily constructed from a grid of theoretical continuum-normalized spectra. In later sub-sections we discuss the specific choices made for the \emph{APOGEE} and \emph{BOSS} applications, but for now we discuss some general principles and practices that we found successful. The objective function to minimize
\begin{align}
    \chi^2 = ||\vecX - \vecW\vecH||^2
\end{align}
\noindent{}is the Frobenius norm (the squared approximation error). This can be solved through coordinate descent, or through the multiplicative update rules developed by \citet{Lee:2000}:
\begin{align}
    \vecW_{ij}^{\mathrm{(new)}} \leftarrow \vecW_{ij} \frac{\left[\vecX\vecH\transpose\right]_{ij}}{\left[\vecW\vecH\vecH\transpose\right]_{ij}} \quad \phantom{.}\\
    \vecH_{ij}^{\mathrm{(new)}} \leftarrow \vecH_{ij} \frac{\left[\vecW\transpose\vecX\right]_{ij}}{\left[\vecW\transpose\vecW\vecH\right]_{ij}} \quad .
\end{align}

We used the \texttt{scipy.decomposition.NMF} implementation \citep{scipy} with some minor adjustments to minimize the memory requirements (e.g., allowing for lower precision float types). We used no regularization on $\vecW$ or $\vecH$. We found substantial improvements by initializing $\vecH$ with non-negative double singular value decomposition with zeros filled-in with the average of $\vecX$ (the default behavior in the \texttt{scipy} implementation). Initializing with random or small non-negative values produced comparable results but required many more iterations. We stopped the approximation of $\vecW$ and $\vecH$ after 1,000 iterations of multiplicative updates. This took minutes to hours to complete, depending on the size of the theoretical grid $\vecX$.\\


There are no requirements on the number of grid dimensions (e.g., whether or not to include $[\alpha/\mathrm{Fe}]$, $[\mathrm{C/Fe}]$), and no strict requirements (see Section~\ref{sec:discussion}) on the spacing between grid points. The only implicit requirement is that the grid should approximately span the range of stars that you intent to apply the method.\footnote{This is more of a recommendation than a requirement. For example, in practice we found that a grid trained on OBA-type stars was also sufficient to model many white dwarf spectra.} For this reason, we chose to include as many spectra as our memory constraints would allow. If the number of spectra exceeded memory constraints, we would opt to skip every $n$th theoretical spectrum.\\



%Note that the stellar parameters used to compute the theoretical spectra are ignored when solving for $\vecW$ and $\vecH$ in Equation~\ref{eq:nmf}: only the spectra are used. There are correlations between the stellar parameters and the amplitudes $\vecW$, but as we discuss in Section~\ref{sec:discussion}, we avoid trying to use the amplitudes $\vecW$ (or $\vecpsi$) to interpreting stellar parameters.  


%\subsection{Inference}

The method can be readily applied to real data once the eigenspectra $\vecH$ have been computed. We experimented with choices of initialisation and inference. Since both components are linear and have closed-form solutions, we did find some success by alternating between solving for $\vecpsi$ and $\vectheta$, but ultimately chose to optimize all parameters $\{\vecpsi,\vectheta\}$ simultaneously. The number of parameters scales as $C + n_\textrm{regions}(2n_\textrm{degree} + 1)$: $C$ amplitudes for $C$ eigenspectra ($\vecpsi$), and $2n_\textrm{degree} + 1$ sine-and-cosine coefficients ($\vectheta$) per chosen continuum region (e.g., per chip). Initializing from small ($10^{-12}$) values of $\vecpsi$ and a closed-form solution of $\vectheta$ (conditioned on small $\vecpsi$) seemed to work well in many scenarios. \\

\subsection{APOGEE spectra}
\label{sec:apogee}

The \apogee\ spectrograph spans approximately 1.5 to 1.7\,microns and has a resolving power of about 22,500 \citep{wilson}. There are three chips, which we chose to model as separate continuum regions (each with their own continuum coefficients $\vecpsi$). We used a 3rd order sine-and-cosine basis with length scale $L = 1300$, corresponding to 7 continuum coefficients per chip.\\

We use the \todo{DR17 grids computed for FERRE, some in non-LTE}. The data are assumed to be resampled onto a rest-frame wavelength sampling that is uniform in log-spacing. We took the high-resolution theoretical grids and convolved them to a spectral resolution of 22,500 and sampled it to match the data. In total there are nearly \todo{1 M} spectra each with 8,575 pixels. This proved too large to fit in memory when computing the eigenspectra $\vecH$, forcing us to skip every 10th spectra.\\

We chose to use 32 eigenspectra to construct the line absorption model, some of which are shown in Figure~\ref{fig:schematic}. During training, the hydrogen lines separate out immediately before other absorption features are recognizable. After training is complete, there are multiple eigenspectra that only have hydrogen lines: some narrow, and some broad. Eigenspectra with only recognizable patterns of molecular features are also present. The remaining eigenspectra have line absorption at many different wavelengths, representing something akin to overall metallicity. \\

With these eigenspectra and continuum settings, we simultaneously fit line absorption and the joint continuum-instrument response for all \apogee\ spectra in SDSS-IV DR17 \citep{dr17} and \apogee\ spectra acquired by the Milky Way Mapper (MWM) in SDSS-V \citep{kollmeier}. In total there are \todo{X} \apogee\ spectra fit by this process: \todo{X} combined spectra and \todo{X} individual visits. We fit combined (co-added) spectra first, and for individual visit spectra we analytically computed the continuum coefficients $\vectheta$ for that visit, conditioned on the $\vecpsi$ found from the combined fit. In practice this implies that the combined and visit spectra for the same star share the same amplitudes $\vecpsi$ for computing rectified flux, but may have different continuum coefficients per visit. Figure~\ref{fig:example-apogee-spectra} show example \apogee\ rectified spectra for different spectral types. 

\begin{figure}
    \caption{Example rectified \apogee\ spectra (black) and best-fitting rectified line absorption (red) for different spectral types. \label{fig:example-apogee-spectra}}
\end{figure}


\subsection{Application to BOSS spectra}
\label{sec:}

SDSS-V is acquiring optical spectra for millions of Milky Way stars using the \boss\ 
low-resolution ($\mathcal{R} \sim 4{,}000$) spectrograph at Apache Point Observatory (APO) and Las Campanas Observatory (LCO). Although the highest priority targets observed by \boss\ tend to be Black Hole Mapper (BHM) targets, many Milky Way stars are included per field, either because they are specifically targeted as part of a Milky Way Mapper carton\footnote{`Cartons' are how targets are assigned in SDSS-V.}, or because they are standard stars.\\

We took all \boss\ spectra where the source was allocated to a Milky Way Mapper carton, or spectra where the source was allocated to a stellar-like carton. \todo{Some description of stellar-like.} For these spectra we fit the line absorption and joint continuum-instrument response.\\

We used the BOSZ spectral grid to construct eigenspectra for a line absorption model for \boss\ spectra. This is a high-resolution ($\mathcal{R} \sim 300{,}000$) finely sampled theoretical grid that spans a sufficiently large range in stellar parameters. We convolved the spectra to the nominal mean resolving power of the \boss\ spectrograph ($\mathcal{R} \sim 4{,}000$) and resampled it to the same pixels that the \boss\ spectra are resampled to: a uniform-in-log wavelength sampling from approximately 360\,nm until \todo{1000\,nm}. We clipped any BOSZ flux values with emission lines exceeding normalized flux values of 1, as they would violate our NMF requirements. As we discuss in later sections, any emission lines are sufficiently rare in stellar spectra that they do not contribute significantly to the $\chi^2$ at inference time.\\

We used all \todo{X} BOSZ spectra to construct a line absorption matrix $\vecX$ and find the approximating matrices $\vecW$ and $\vecH$. For fitting \boss\ spectra, we separated the continuum fits into two chunks to represent the blue and red chip of the detector. Most of the time the blue and red chips are correctly merged together to produce a seamless contiguous spectrum from \todo{300\,nm to 1000,\nm}, but we noticed there could some times be jumps in the spectrum due to an imperfect merge. For these reasons, we used two sections from \todo{3750\,\AA\ to 6000\,\AA and 6050\,\AA to 10000\,\AA}. We chose a \todo{3rd} order sines-and-cosines basis with a length scale $L = X$\,\AA. Following what we did with \apogee\ spectra, we fit the combined \boss\ spectrum first for a given star, and then analytically computed $\vectheta{}|\vecpsi$: the continuum coefficients $\vectheta$ for the given visit, conditioned on the amplitudes optimized from the combined spectrum $\vecpsi$.

\section{Results}\label{sec:results}

\begin{figure}
    \caption{A Gaia H-R diagram showing the median $\chi^2$ per bin.}
\end{figure}

\begin{figure}
    \caption{Median observed rectified flux values as a function of S/N ratio showing that there is no bias as a function of S/N ratio.}
\end{figure}


\begin{figure}
    \caption{Median model continuum-rectified flux values as a function of effective temperature, compared to the median observed rectified flux values as a function of Gaia colours, showing there is no bias as a function of spectral type?}
\end{figure}


\begin{figure}
    \caption{Gaia H-R diagram with each bin coloured by the median $\vecpsi_i$ value, picking some $i$th eigenspectra that looks like metallicity. \label{fig:gaia_hrd_metallicity}}
\end{figure}


\begin{figure*}
    \caption{The median pixel $\chi^2$ value as a function of \emph{Gaia} $\mathrm{BP} - \mathrm{RP}$ color for main-sequence stars observed with the BOSS spectrograph. \todo{Should expect to see increasing residuals due to TiO bands not captured by the model, and emission lines.}}
    % Could do the same showing standard deviation per bin
\end{figure*}


- figure showing the 

\section{Discussion}\label{sec:discussion}

- getting good fits even with bad models: the hydrogen lines in the apogee grid are knowingly incorrect, but we see no difference in the $\chi^2$ at those wavelengths. That is because the hydrogen lines separate out into very clean eigenspectra, where the amplitude for that eigenspectra is not required to vary smoothly with other amplitudes

- biases as a function of S/N or labels

- poorly modelled regions as a function of pixel

- $\chi^2$ across the HRD

- excellent way to identify regions where models and observations may disagree

- orthogonality of the two model components

- issue with M-giant eigenspectra when they were over-represented in the sample


\section{Conclusions}\label{sec:conclusions}

We solved continuum normalization.


Code is available at \url{https://github.com/andycasey/continuum} and is registered in the Python Package Index as \texttt{stellar-continuum}. 

\paragraph{Software}
\texttt{numpy} \citep{numpy} ---
\texttt{matplotlib} \citep{matplotlib} ---
\texttt{scipy} \citep{scipy}.

\paragraph{Acknowledgements}
It is a pleasure to thank
% All these people are possible co-authors
    Adam Wheeler (Ohio State University),
    Andrew Saydjari (Harvard),
    David W. Hogg (New York University),
    Megan Bedell (Flatiron Institute),
.
% include bibliography
\bibliographystyle{aasjournal}
%\bibliography{bibliography}

\end{document}
